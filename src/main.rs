use aichat_cli::{
    chat_message::{self, Role},
    claude_client,
    model::{Campany, Model},
    openai_client,
};
use anyhow::Result;
use dotenv::dotenv;
use requestty::Question;
use std::env;

fn main() {
    // ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’å–å¾—
    let args: Vec<String> = env::args().collect();

    // -t ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèª
    let is_translation_mode = args.contains(&"-t".to_string());

    match run(is_translation_mode) {
        Ok(_) => {}
        Err(e) => {
            eprintln!("error: {}", e);
            std::process::exit(1);
        }
    };
}

fn run(is_translation_mode: bool) -> Result<()> {
    // å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’ã“ã“ã§ç¢ºèª
    dotenv().ok();
    let openai_token =
        env::var("OPENAI_API_KEY").expect("ç’°å¢ƒå¤‰æ•°ã«OPENAI_API_KEYã‚’ã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„");
    let anthropic_token =
        env::var("ANTHROPIC_API_KEY").expect("ç’°å¢ƒå¤‰æ•°ã«ANTHROPIC_API_KEYã‚’ã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„");

    if is_translation_mode {
        println!("ğŸ“– ç¿»è¨³ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã—ã¾ã™");
    }

    let mut gpt_client = openai_client::ChatGPTClient::new(openai_token);
    let mut claude_client = claude_client::ClaudeClient::new(anthropic_token);

    // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã•ã›ã‚‹
    let selected_model = select_model_input(&claude_client, &gpt_client)?;

    // åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    let mut messages = chat_message::MessageHistory::default();

    match selected_model.campany {
        Campany::Claude => {
            claude_client.set_model(selected_model.name.to_owned());

            if is_translation_mode {
                let initial_message = "system: ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ç¿»è¨³ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ãŒæ—¥æœ¬èªãªã‚‰è‹±è¨³ã‚’å‡ºåŠ›ã—ã€è‹±èªãªã‚‰æ—¥æœ¬èªè¨³ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„";
                messages.push(Role::User, initial_message);
                messages.push(
                    Role::Assistant,
                    "ã¯ã„ã€æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚ã“ã“ã‹ã‚‰ã¯ç¿»è¨³ãƒ¢ãƒ¼ãƒ‰ã§å¿œå¯¾ã—ã¾ã™ã€‚",
                );
            }
            claude_client.run_claude(messages)?;
        }
        Campany::OpenAI => {
            gpt_client.set_model(selected_model.name.to_owned());

            if is_translation_mode {
                let initial_message = "ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ç¿»è¨³ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ãŒæ—¥æœ¬èªãªã‚‰è‹±è¨³ã‚’å‡ºåŠ›ã—ã€è‹±èªãªã‚‰æ—¥æœ¬èªè¨³ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„";
                messages.push(Role::System, initial_message);
            }

            gpt_client.run_chatgpt(messages)?;
        }
    }

    Ok(())
}

/// ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã•ã›ã‚‹
fn select_model_input(
    claude_client: &claude_client::ClaudeClient,
    gpt_client: &openai_client::ChatGPTClient,
) -> Result<Model, anyhow::Error> {
    let claude_models = claude_client.get_model_list();
    let openai_models = gpt_client.fetch_models()?;
    let models: Vec<Model> = claude_models
        .iter()
        .chain(openai_models.iter())
        .cloned()
        .collect();
    let select = Question::select("theme")
        .should_loop(false)
        .message("ğŸ¤– åˆ©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ (Ctrl+c to exit)")
        .choices(models.clone())
        .default(0)
        .build();
    let answer = requestty::prompt_one(select)?;
    let model_index = &answer.as_list_item().unwrap().index;
    let model = models[*model_index].clone();
    Ok(model.to_owned())
}
